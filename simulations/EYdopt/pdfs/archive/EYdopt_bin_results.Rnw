\documentclass[11pt]{article}
%\usepackage[showframe]{geometry}
\usepackage[table]{xcolor}
\usepackage{caption}
\usepackage{lscape,verbatim,mathrsfs}
\usepackage{graphics,amsmath,pstricks}
\usepackage{amssymb,enumerate}
\usepackage{amsbsy,amsmath,amsthm,amsfonts, amssymb}
\usepackage{graphicx, rotate, array}
\usepackage{geometry,multirow}
\usepackage{color,soul}
\usepackage{float}
%\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
%\renewcommand{\baselinestretch}{1.9}
\usepackage{tcolorbox}
\renewcommand{\familydefault}{cmss}
\textwidth=6.65in \textheight=9.7in
\parskip=.025in
\parindent=0in
\oddsidemargin=-0.1in \evensidemargin=-.1in \headheight=-.6in
\footskip=0.5in \DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}


\begin{document}

The purpose of this simulation study is to examine different ways of evaluating an ODTR. Specifically, we examine how different estimators (g-comp, IPTW, IPTW-DR, TMLE, CV-TMLE) do with approximating the statistical parameter $E_0[Y_{d_0}]$ and the data-adaptive parameter $E_0[Y_{d_n}]$, under different SL ODTR estimators (ie different SL libraries, varying in ``aggressiveness"). Here, $d_0$ is true optimal rule, $d_n$ is estimate of optimal rule.


<<libs_and_funs, echo = F, message = F,warning = F>>=
library(dplyr)
library(ggplot2)
library(xtable)
library(ggpubr)
library('latex2exp')
#source("/Users/linamontoya/Box Sync/Dissertation/R/7showResults.R")
#source('/Users/linamontoya/Box Sync/Dissertation/R/1DGPfunctions.R')
@







\section{Description of DGP}

\begin{align*}
W_1,W_2,W_3,W_4 &\sim Normal(\mu=0,\sigma^2=1) \\
A &\sim Bernoulli(p=0.5) \\
Y &\sim Bernoulli(p) \text{ .}\\
\end{align*}
\begin{align*}
p &= 0.5*logit^{-1} (1-W_1^2  + 3W_2  + 5W_3^2 A - 4.45A)+0.5logit^{-1} (-0.5- W_3  + 2W_1 W_2  + 3|W2|A - 1.5A) \text{ ,}
\end{align*}
True blip function is:
\begin{align*}
B_0 (W)= & 0.5[logit^{-1} (1-W_1^2  + 3W_2  + 5W_3^2  - 4.45)+logit^{-1} (-0.5- W_3  + 2W_1 W_2  + 3|W2|  - 1.5)\\
& - logit^{-1} (1-W_1^2  + 3W_2 )+logit^{-1} (-0.5- W_3  + 2W_1 W_2 )] \text{ .}
\end{align*}



<<DGP_bin, echo = F>>=
# load DGP_bin stuff
load(file = "/Users/linamontoya/Box Sync/Dissertation/SL.ODTR/simulations/true values/DGP_bin_complex_true_values.RData")
@

<<EYdopt_DGP_AL_bin_step1a, echo = F, results = 'asis', warning = F, fig.height=5.5, fig.width=12>>=
load(file = "/Users/linamontoya/Box Sync/Dissertation/simulations/EYdopt/results/EYdopt_bin_simple.RData")
#load(file = "/Users/linamontoya/Box Sync/Dissertation/simulations/EYdopt/results/EYdopt_bin_medium.RData")
load(file = "/Users/linamontoya/Box Sync/Dissertation/simulations/EYdopt/results/EYdopt_bin_medium_HAL.RData")
#load(file = "/Users/linamontoya/Box Sync/Dissertation/simulations/EYdopt/results/EYdopt_bin_most.RData")
load(file = "/Users/linamontoya/Box Sync/Dissertation/simulations/EYdopt/results/EYdopt_bin_most_HAL.RData")

#results = EYdopt_bin_simple

#E0Yd0
E0Yd0 = DGP_bin_complex_true_values$EYd_star

make_table_fun = function(results) {

  #EnYdn
  EnYdn = do.call('rbind', lapply(results, function(x) x$EnYdn[grep("Psi_", rownames(x$EnYdn)), "EYd"]))
  colnames(EnYdn) = row.names(results[[1]]$EnYdn)[grep("Psi_", rownames(results[[1]]$EnYdn))]
  CI_EnYdn = do.call('rbind', lapply(results, function(x) x$EnYdn[grep("CI_", rownames(x$EnYdn)), "EYd"]))
  colnames(CI_EnYdn) = row.names(results[[1]]$EnYdn)[grep("CI_", rownames(results[[1]]$EnYdn))]

  #EnYdn for E0Yd0
  bias_EnYdn_for_E0Yd0 = colMeans(EnYdn - E0Yd0)
  var_EnYdn_for_E0Yd0 = diag(var(EnYdn))
  MSE_EnYdn_for_E0Yd0 = bias_EnYdn_for_E0Yd0^2 + var_EnYdn_for_E0Yd0
  coverage_EnYdn_for_E0Yd0 = colMeans(E0Yd0 > CI_EnYdn[,grep("1", colnames(CI_EnYdn))] & E0Yd0 < CI_EnYdn[,grep("2", colnames(CI_EnYdn))])
  table_EnYdn_for_E0Yd0 = data.frame(Estimator = c("Unadjusted", "G-comp.", "IPTW", "IPTW-DR", "TMLE", "CV-TMLE"),
                                     Bias = bias_EnYdn_for_E0Yd0,
                                     Variance = var_EnYdn_for_E0Yd0,
                                     MSE = MSE_EnYdn_for_E0Yd0,
                                     Coverage = NA)
  table_EnYdn_for_E0Yd0 = round(table_EnYdn_for_E0Yd0[,sapply(table_EnYdn_for_E0Yd0, class) == "numeric"], 4)
  names(coverage_EnYdn_for_E0Yd0) = paste0("Psi_", substr(names(coverage_EnYdn_for_E0Yd0), start = 4, stop = nchar(names(coverage_EnYdn_for_E0Yd0))-1))
  table_EnYdn_for_E0Yd0[names(coverage_EnYdn_for_E0Yd0),"Coverage"] = coverage_EnYdn_for_E0Yd0
  table_EnYdn_for_E0Yd0$Coverage = paste0(table_EnYdn_for_E0Yd0$Coverage*100, "%")
  table_EnYdn_for_E0Yd0$Coverage[table_EnYdn_for_E0Yd0$Coverage == "NA%"] = "-"
  table_EnYdn_for_E0Yd0 = table_EnYdn_for_E0Yd0[-which(rownames(table_EnYdn_for_E0Yd0) == "Psi_unadj"),]

  #EnYd0
  EnYd0 = do.call('rbind', lapply(results, function(x) x$EnYd0[grep("Psi_", rownames(x$EnYd0)), "EYd"]))
  colnames(EnYd0) = row.names(results[[1]]$EnYd0)[grep("Psi_", rownames(results[[1]]$EnYd0))]
  CI_EnYd0 = do.call('rbind', lapply(results, function(x) x$EnYd0[grep("CI_", rownames(x$EnYd0)), "EYd"]))
  colnames(CI_EnYd0) = row.names(results[[1]]$EnYd0)[grep("CI_", rownames(results[[1]]$EnYd0))]

  #EnYd0 for E0Yd0
  bias_EnYd0_for_E0Yd0 = colMeans(EnYd0 - E0Yd0)
  var_EnYd0_for_E0Yd0 = diag(var(EnYd0))
  MSE_EnYd0_for_E0Yd0 = bias_EnYd0_for_E0Yd0^2 + var_EnYd0_for_E0Yd0
  coverage_EnYd0_for_E0Yd0 = colMeans(E0Yd0 > CI_EnYd0[,grep("1", colnames(CI_EnYd0))] & E0Yd0 < CI_EnYd0[,grep("2", colnames(CI_EnYd0))])
  table_EnYd0_for_E0Yd0 = data.frame(Estimator = c("Unadjusted", "G-comp.", "IPTW", "IPTW-DR", "TMLE", "CV-TMLE"),
                                     Bias = bias_EnYd0_for_E0Yd0,
                                     Variance = var_EnYd0_for_E0Yd0,
                                     MSE = MSE_EnYd0_for_E0Yd0,
                                     Coverage = NA)
  table_EnYd0_for_E0Yd0 = round(table_EnYd0_for_E0Yd0[,sapply(table_EnYd0_for_E0Yd0, class) == "numeric"], 4)
  names(coverage_EnYd0_for_E0Yd0) = paste0("Psi_", substr(names(coverage_EnYd0_for_E0Yd0), start = 4, stop = nchar(names(coverage_EnYd0_for_E0Yd0))-1))
  table_EnYd0_for_E0Yd0[names(coverage_EnYd0_for_E0Yd0),"Coverage"] = coverage_EnYd0_for_E0Yd0
  table_EnYd0_for_E0Yd0$Coverage = paste0(table_EnYd0_for_E0Yd0$Coverage*100, "%")
  table_EnYd0_for_E0Yd0$Coverage[table_EnYd0_for_E0Yd0$Coverage == "NA%"] = "-"
  table_EnYd0_for_E0Yd0 = table_EnYd0_for_E0Yd0[-which(rownames(table_EnYd0_for_E0Yd0) == "Psi_unadj"),]

  #E0Ydn
  E0Ydn = do.call('rbind', lapply(results, function(x) x$E0Ydn[, "EYd"]))
  colnames(E0Ydn) = row.names(results[[1]]$E0Ydn)

  #EnYdn for E0Ydn
  bias_EnYdn_for_E0Ydn = colMeans(EnYdn[,-grep("CV.TMLE", colnames(EnYdn))] - E0Ydn[,"E0Ydn.nonCVTMLE"])
  bias_EnYdn_for_E0Ydn = c(bias_EnYdn_for_E0Ydn, Psi_CV.TMLE = mean(EnYdn[,grep("CV.TMLE", colnames(EnYdn))] - E0Ydn[,"E0Ydn.CVTMLE"]))
  var_EnYdn_for_E0Ydn = diag(var(EnYdn))
  MSE_EnYdn_for_E0Ydn = bias_EnYdn_for_E0Ydn^2 + var_EnYdn_for_E0Ydn
  CI_EnYdn1 = CI_EnYdn[,grep("1", colnames(CI_EnYdn))]
  CI_EnYdn2 = CI_EnYdn[,grep("2", colnames(CI_EnYdn))]
  coverage_EnYdn_for_E0Ydn = colMeans(E0Ydn[,"E0Ydn.nonCVTMLE"] > CI_EnYdn1[,-grep("CV.TMLE", colnames(CI_EnYdn1))] & E0Ydn[,"E0Ydn.nonCVTMLE"] < CI_EnYdn2[,-grep("CV.TMLE", colnames(CI_EnYdn2))])
  coverage_EnYdn_for_E0Ydn = c(coverage_EnYdn_for_E0Ydn, CI_CV.TMLE1 = mean(E0Ydn[,"E0Ydn.CVTMLE"] > CI_EnYdn1[,grep("CV.TMLE", colnames(CI_EnYdn1))] & E0Ydn[,"E0Ydn.CVTMLE"] < CI_EnYdn2[,grep("CV.TMLE", colnames(CI_EnYdn2))]))
  table_EnYdn_for_E0Ydn = data.frame(Estimator = c("Unadjusted", "G-comp.", "IPTW", "IPTW-DR", "TMLE", "CV-TMLE"),
                                     Bias = bias_EnYdn_for_E0Ydn,
                                     Variance = var_EnYdn_for_E0Ydn,
                                     MSE = MSE_EnYdn_for_E0Ydn,
                                     Coverage = NA)
  table_EnYdn_for_E0Ydn = round(table_EnYdn_for_E0Ydn[,sapply(table_EnYdn_for_E0Ydn, class) == "numeric"], 4)
  names(coverage_EnYdn_for_E0Ydn) = paste0("Psi_", substr(names(coverage_EnYdn_for_E0Ydn), start = 4, stop = nchar(names(coverage_EnYdn_for_E0Ydn))-1))
  table_EnYdn_for_E0Ydn[names(coverage_EnYdn_for_E0Ydn),"Coverage"] = coverage_EnYdn_for_E0Ydn
  table_EnYdn_for_E0Ydn$Coverage = paste0(table_EnYdn_for_E0Ydn$Coverage*100, "%")
  table_EnYdn_for_E0Ydn$Coverage[table_EnYdn_for_E0Ydn$Coverage == "NA%"] = "-"
  table_EnYdn_for_E0Ydn = table_EnYdn_for_E0Ydn[-which(rownames(table_EnYdn_for_E0Ydn) == "Psi_unadj"),]

  return(list(table_EnYdn_for_E0Yd0 = table_EnYdn_for_E0Yd0,
              table_EnYd0_for_E0Yd0 = table_EnYd0_for_E0Yd0,
              table_EnYdn_for_E0Ydn = table_EnYdn_for_E0Ydn))
}

make_table_fun(results = EYdopt_bin_simple)
make_table_fun(results = EYdopt_bin_medium)
make_table_fun(results = EYdopt_bin_most)


@

\section{Library legend}

\begin{itemize}
\item Incorrect GLM
\begin{itemize}
\item QAW.SL.library = linear model main terms W and A and interaction with W and A
\item blip.SL.library = linear model with main terms W
\end{itemize}
\item GLMs
\begin{itemize}
\item QAW.SL.library = linear model with $W_j$ and A as main terms and $W_j$*A interaction for each $j$
\item blip.SL.library = linear model with main terms $W_j$ for each $j$
\end{itemize}
\item ML + GLMs not aggressive
\begin{itemize}
\item QAW.SL.library = GLMs library AND SL.glm, SL.mean, SL.glm.interaction, SL.earth, SL.nnet, SL.svm, SL.rpart
\item blip.SL.library = GLMs library AND SL.glm, SL.mean, SL.glm.interaction, SL.earth, SL.nnet, SL.svm, SL.rpart
\end{itemize}
\item ML + GLMs not aggressive
\begin{itemize}
\item QAW.SL.library = ML + GLMs aggressive library AND SL.randomForest
\item blip.SL.library = ML + GLMs aggressive library AND SL.randomForest
\end{itemize}
\end{itemize}

\section{Results}
<<echo = F, warning = F>>=
#results = EYdopt_bin_simple
#title = "test"

make_plot_fun = function(results, title) {

  #EnYdn
  EnYdn = do.call('rbind', lapply(results, function(x) x$EnYdn[grep("Psi_", rownames(x$EnYdn)), "EYd"]))
  colnames(EnYdn) = row.names(results[[1]]$EnYdn)[grep("Psi_", rownames(results[[1]]$EnYdn))]
  EnYdn = EnYdn[,-which(colnames(EnYdn) == "Psi_unadj")]
  df_dn = data.frame(Estimator = factor(colnames(EnYdn), levels = colnames(EnYdn)),
                     Estimates = colMeans(EnYdn),
                     minQ = apply(EnYdn, 2, quantile, probs = 0.025),
                     maxQ = apply(EnYdn, 2, quantile, probs = 0.975),
                     d_type = "dn")
  #EnYd0
  EnYd0 = do.call('rbind', lapply(results, function(x) x$EnYd0[grep("Psi_", rownames(x$EnYd0)), "EYd"]))
  colnames(EnYd0) = row.names(results[[1]]$EnYd0)[grep("Psi_", rownames(results[[1]]$EnYd0))]
  EnYd0 = EnYd0[,-which(colnames(EnYd0) == "Psi_unadj")]
  df_d0 = data.frame(Estimator = factor(colnames(EnYd0), levels = colnames(EnYd0)),
                     Estimates = colMeans(EnYd0),
                     minQ = apply(EnYd0, 2, quantile, probs = 0.025),
                     maxQ = apply(EnYd0, 2, quantile, probs = 0.975),
                     d_type = "d0")
  df = rbind(df_d0, df_dn)
  df$Estimator = factor(as.character(df$Estimator), labels = c("G-comp.", "IPTW", "IPTW-DR", "TMLE", "CV-TMLE"), levels = c("Psi_gcomp", "Psi_IPTW", "Psi_IPTW_DR", "Psi_TMLE", "Psi_CV.TMLE"))

  #E0Ydn
  E0Ydn = do.call('rbind', lapply(results, function(x) x$E0Ydn[, "EYd"]))
  colnames(E0Ydn) = row.names(results[[1]]$E0Ydn)

  pd <- position_dodge(width = 0.7)
  df %>%
    ggplot(aes(x = Estimator, y = Estimates, group = d_type)) +
    geom_point(aes(shape = d_type), position = pd, size = 2.5) +
    geom_line(aes(y = 0, colour = "red")) +
    geom_line(aes(y = 0, colour = "blue")) +
    geom_line(aes(y = 0, colour = "black")) +
    scale_shape_discrete(name = "        Rule: ", labels = c("dn" = "Estimated ODTR",
                                                             "d0" = "True ODTR")) +
    scale_colour_manual(name = "         Statistical Estimand:",
                        values = c("black", "blue", "red"),
                        labels = c("red" = parse(text = TeX('$\\Psi_{d^*_{n,CV}}$')),
                                   "blue" = parse(text = TeX('$\\Psi_{d^*_n}$')),
                                   "black" = parse(text = TeX('$\\Psi_{d^*_0}$'))))+
    ylim(c(.38,.97)) +
    geom_errorbar(aes(ymin = minQ, ymax = maxQ), width = .4, position = pd) +
    geom_hline(yintercept = mean(E0Ydn[,"E0Ydn.CVTMLE"]), colour = "red")+#, alpha=I(0.1)) +
    geom_hline(yintercept = mean(E0Ydn[,"E0Ydn.nonCVTMLE"]), colour = "blue")+#, alpha=I(0.1)) +
    geom_hline(yintercept = E0Yd0, colour = "black") +
    scale_x_discrete(name ="Estimator") +
    ylab("") +
    ggtitle(title) +
    theme_bw() +
    theme(text = element_text(size=18), panel.border = element_blank())

}

plot1 = make_plot_fun(results = EYdopt_bin_simple, title = "GLMs - least data adaptive")
#plot2 = make_plot_fun(results = EYdopt_bin_medium, title = "ML + GLMs - medium data adaptive")
plot2 = make_plot_fun(results = EYdopt_bin_medium, title = "ML + HAL + GLMs - medium data adaptive")
#plot3 = make_plot_fun(results = EYdopt_bin_most, title = "ML + GLMs - most data adaptive")
plot3 = make_plot_fun(results = EYdopt_bin_most, title = "ML + HAL + GLMs - most data adaptive")
































plot2 = make_plot_EYdopt(EYdopt = EYdoptbin_MLnotaggglms, truevalues = DGP_bin_complex_true_values, title = "ML + GLMs - medium data adaptive")
plot3 = make_plot_EYdopt(EYdopt = EYdoptbin_MLaggglms, truevalues = DGP_bin_complex_true_values, title = "ML + GLMs - most data adaptive")

ggarrange(plot1, plot2, plot3, ncol=3, nrow=1, common.legend = TRUE, legend="bottom")

png(filename="/Users/linamontoya/Box Sync/Dissertation/simulations/EYdopt/plotfig_2.png", width = 1300, height = 850)

ggarrange(plot1, plot2, plot3, ncol=3, nrow=1, common.legend = TRUE, legend="bottom")

dev.off()

@

<<>>=
make_table_EYdopt(EYdopt = EYdoptbin_glms, truevalues = DGP_bin_complex_true_values)
make_table_EYdopt(EYdopt = EYdoptbin_MLnotaggglms, truevalues = DGP_bin_complex_true_values)
make_table_EYdopt(EYdopt = EYdoptbin_MLaggglms, truevalues = DGP_bin_complex_true_values)

@

<<echo = F>>=
# Incorrect GLM
#make_table_EYdopt(EYdopt = EYdoptbin_glm, truevalues = DGP_bin_complex_true_values)


table_all = rbind(data.frame(Library = "GLMs", make_table_EYdopt(EYdopt = EYdoptbin_glms, truevalues = DGP_bin_complex_true_values)),
              data.frame(Library = "ML + GLMs not aggressive", make_table_EYdopt(EYdopt = EYdoptbin_MLnotaggglms, truevalues = DGP_bin_complex_true_values)),
              data.frame(Library = "ML + GLMs aggressive", make_table_EYdopt(EYdopt = EYdoptbin_MLaggglms, truevalues = DGP_bin_complex_true_values)))
table_all = table_all[-grep("LTMLE", rownames(table_all)),]

table_sampspec = table_all[grep("_sampspec", table_all$Estimator),]
table_sampspec$Estimator = substr(table_sampspec$Estimator, start = 1, stop = unlist(gregexpr(pattern = "sampspec", table_sampspec$Estimator))-2)
table_sampspec$Estimator = factor(as.character(table_sampspec$Estimator), labels = c("G-comp.", "IPTW", "IPTW-DR", "TMLE", "CV-TMLE"), levels = c("gcomp", "IPTW", "IPTW_DR", "TMLE", "CV.TMLE"))
table_sampspec$Comparison = "EnYdn for E0Ydn"
table_sampspec = table_sampspec[,c("Comparison", "Library", "Estimator", "Bias", "Variance", "MSE", "Coverage")]
table_sampspec

write.csv(table_sampspec, file = "/Users/linamontoya/Box Sync/Dissertation/simulations/EYdopt/results/table_sampspec2.csv", row.names = F)

table_dopt0 = table_all[grep("_dopt0", table_all$Estimator),]
table_dopt0$Estimator = substr(table_dopt0$Estimator, start = 1, stop = unlist(gregexpr(pattern = "dopt0", table_dopt0$Estimator))-2)
table_dopt0$Estimator = factor(as.character(table_dopt0$Estimator), labels = c("G-comp.", "IPTW", "IPTW-DR", "TMLE", "CV-TMLE"), levels = c("gcomp", "IPTW", "IPTW_DR", "TMLE", "CV.TMLE"))
table_dopt0$Comparison = "EnYd0 for E0Yd0"
table_dopt0 = table_dopt0[,c("Comparison", "Library", "Estimator", "Bias", "Variance", "MSE", "Coverage")]
table_dopt0

write.csv(table_dopt0, file = "/Users/linamontoya/Box Sync/Dissertation/simulations/EYdopt/results/table_dopt0_2.csv", row.names = F)

table_dn = table_all[-c(grep("_dopt0", table_all$Estimator), grep("_sampspec", table_all$Estimator)),]
#table_dn$Estimator = substr(table_dn$Estimator, start = 1, stop = unlist(gregexpr(pattern = "dopt0", table_dn$Estimator))-2)
table_dn$Estimator = factor(as.character(table_dn$Estimator), labels = c("G-comp.", "IPTW", "IPTW-DR", "TMLE", "CV-TMLE"), levels = c("gcomp", "IPTW", "IPTW_DR", "TMLE", "CV.TMLE"))
table_dn$Comparison = "EnYdn for E0Yd0"
table_dn = table_dn[,c("Comparison", "Library", "Estimator", "Bias", "Variance", "MSE", "Coverage")]
table_dn

write.csv(table_dn, file = "/Users/linamontoya/Box Sync/Dissertation/simulations/EYdopt/results/table_dn_2.csv")

table_full = rbind(table_dopt0, table_dn, table_sampspec)
write.csv(table_full, file = "/Users/linamontoya/Box Sync/Dissertation/simulations/EYdopt/results/table_full_2.csv", row.names = F)
@

\section{Summary of Results Above}

\begin{itemize}
    \item \underline{\textbf{$E_n[Y_{d_0}]$ to estimate $E_0[Y_{d_0}]$}}: these results speak to performance of estimators of $E_0[Y_{d}]$ for some given $d$ (i.e., not about how well we estimate the rule, but about how well we estimate the performance of a given rule, which here it happens to be $d_0$). Estimator results:
    \begin{itemize}
        \item g-comp: biased
        \begin{itemize}
            \item Note: this differs from estimation of, e.g., $E[Y_1]$ in RCT (or using any treatment rule that isn't a function of covariates), where g-comp using a misspecified glm is a TMLE, and therefore unbiased
        \end{itemize}
        \item IPTW: less efficient (more variability), including less efficient than unadjusted
        \begin{itemize}
            \item Note: this again differs from estimation of, e.g., $E[Y_1]$ in RCT, where IPTW using estimated weights we gain efficiency
        \end{itemize}
        \item IPTW-DR and TMLE: unbiased, EXCEPT if $Q$ estimated aggressively, then bias enough for coverage to drop to $\sim 90\%$
        \begin{itemize}
            \item Small variance gain compared to unadjusted (though suspect this gain would be bigger if covariates were more predictive of outcome?)
        \end{itemize}
\item CV-TMLE: unbiased (even with more aggressive library for $Q$)
\item Unadjusted: unbiased
\begin{itemize}
    \item Small variance price vs the DR estimators, but without risk of bias due to overfitting $Q$
    \item Very little difference compared to CV-TMLE
\end{itemize}
    \end{itemize}
    \item \underline{\textbf{$E_n[Y_{d_n}]$ to estimate $E_0[Y_{d_0}]$}}: these results speak to not only how good of a job we do evaluating the rule (i.e., as above), but also how well we estimate the rule
    \begin{itemize}
        \item None do well. This is all due to fall off in estimating $d_0$, ie $d_n$ not converging to $d_0$ fast enough.
        \begin{itemize}
            \item See ODTR paper just submitted- how to do a better job on $d_0$ (including at finite sample sizes, even if cant get all the way there, how to get closer)
        \end{itemize}
    \end{itemize}
    \item \textbf{\underline{$E_n[Y_{d_n}]$ to estimate $E_0[Y_{d_n}]$}}: these results are of interest if going after a data-adaptive target parameter
    \begin{itemize}
        \item Note: the estimators are targeting different data adaptive parameters. Data-adaptive parameter here for CV-TMLE is the average of the folds, for the others, it is the $d_n$ learned on the whole sample
        \begin{itemize}
            \item However, here they are pretty similar
        \end{itemize}
        \item There is a real price in bias paid by not using sample splitting to evaluate performance
        \begin{itemize}
            \item For all of the other estimators besides CV-TMLE, will overestimate how well the estimated rule does
            \item As the library used to estimate Q gets more aggressive:
            \begin{itemize}
                \item The estimated rule gets closer to the true rule
                \item The price paid (in terms of bias) by not using CV-TMLE increases
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{itemize}


\noindent Big picture summary:
\begin{itemize}
    \item For large sample sizes, small price and many benefits to using CV-TMLE with aggressive library to estimate $E_0[Y_{d_0}]$ and $E_0[Y_{d_n}]$
    \begin{itemize}
        \item But, is this true if truth is simple? If sample size is small, worry is in that case pay a price. We want a method that as sample size increases, goes towards the more complex; when sample size limited, data can't support, will go towards simple (CI-based ODTR might help here)
    \end{itemize}
\end{itemize}


\end{document}
